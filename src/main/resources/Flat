import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.flink.api.common.ExecutionConfig;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.common.typeutils.TypeSerializer;
import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
import org.apache.flink.core.memory.DataInputView;
import org.apache.flink.core.memory.DataOutputView;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.types.DataType;
import org.apache.flink.types.Row;
import org.bson.Document;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.Serializable;
import java.util.HashMap;
import java.util.Map;

// DeltaRecord class to represent the input data
public record DeltaRecord(
        String key,
        String checksum,
        Map<String, Object> value,
        String businessDate,
        String processedDate,
        Integer version
) implements Serializable {
    private static final ObjectMapper MAPPER = new ObjectMapper();

    // Utility method to create a DeltaRecord from a JSON string value
    public static DeltaRecord fromStringValue(
            String key,
            String checksum,
            String value,
            String businessDate,
            String processedDate,
            Integer version
    ) {
        if (value == null) {
            throw new IllegalArgumentException("Value cannot be null");
        }
        try {
            Map<String, Object> valueMap = MAPPER.readValue(value, Map.class);
            return new DeltaRecord(key, checksum, valueMap, businessDate, processedDate, version);
        } catch (JsonProcessingException e) {
            throw new IllegalArgumentException("Failed to parse value as JSON: " + value, e);
        }
    }
}

// Custom TypeSerializer for Map<String, Object>
class MapSerializer extends TypeSerializer<Map<String, Object>> {
    private static final ObjectMapper MAPPER = new ObjectMapper();

    @Override
    public boolean isImmutableType() {
        return false;
    }

    @Override
    public TypeSerializer<Map<String, Object>> duplicate() {
        return this;
    }

    @Override
    public Map<String, Object> createInstance() {
        return new HashMap<>();
    }

    @Override
    public Map<String, Object> copy(Map<String, Object> from) {
        return new HashMap<>(from);
    }

    @Override
    public Map<String, Object> copy(Map<String, Object> from, Map<String, Object> reuse) {
        reuse.clear();
        reuse.putAll(from);
        return reuse;
    }

    @Override
    public int getLength() {
        return -1; // Variable length
    }

    @Override
    public void serialize(Map<String, Object> value, DataOutputView target) throws IOException {
        if (value == null) {
            throw new IllegalArgumentException("Cannot serialize null Map");
        }
        try {
            byte[] bytes = MAPPER.writeValueAsBytes(value);
            target.writeInt(bytes.length);
            target.write(bytes);
        } catch (JsonProcessingException e) {
            throw new IOException("Failed to serialize Map to JSON bytes: " + value, e);
        }
    }

    @Override
    public Map<String, Object> deserialize(DataInputView source) throws IOException {
        int length = source.readInt();
        byte[] bytes = new byte[length];
        source.readFully(bytes);
        try {
            return MAPPER.readValue(bytes, Map.class);
        } catch (Exception e) {
            throw new IOException("Failed to deserialize bytes to Map", e);
        }
    }

    @Override
    public Map<String, Object> deserialize(Map<String, Object> reuse, DataInputView source) throws IOException {
        reuse.clear();
        Map<String, Object> result = deserialize(source);
        reuse.putAll(result);
        return reuse;
    }

    @Override
    public void copy(DataInputView source, DataOutputView target) throws IOException {
        int length = source.readInt();
        target.writeInt(length);
        byte[] bytes = new byte[length];
        source.readFully(bytes);
        target.write(bytes);
    }

    @Override
    public TypeSerializerSnapshot<Map<String, Object>> snapshotConfiguration() {
        return new MapSerializerSnapshot();
    }
}

// TypeSerializerSnapshot for MapSerializer to handle checkpointing
class MapSerializerSnapshot implements TypeSerializerSnapshot<Map<String, Object>> {
    @Override
    public int getCurrentVersion() {
        return 1;
    }

    @Override
    public void writeSnapshot(DataOutputView out) throws IOException {
        // No state to write
    }

    @Override
    public void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {
        // No state to read
    }

    @Override
    public TypeSerializer<Map<String, Object>> restoreSerializer() {
        return new MapSerializer();
    }

    @Override
    public TypeSerializerSchemaCompatibility<Map<String, Object>> resolveSchemaCompatibility(
            TypeSerializer<Map<String, Object>> newSerializer) {
        if (newSerializer.getClass().equals(MapSerializer.class)) {
            return TypeSerializerSchemaCompatibility.compatibleAsIs();
        } else {
            return TypeSerializerSchemaCompatibility.incompatible();
        }
    }
}

// Custom TypeInformation for Map<String, Object>
class MapTypeInformation extends TypeInformation<Map<String, Object>> {
    private final TypeSerializer<Map<String, Object>> serializer = new MapSerializer();

    @Override
    public boolean isBasicType() {
        return false;
    }

    @Override
    public boolean isTupleType() {
        return false;
    }

    @Override
    public int getArity() {
        return 1;
    }

    @Override
    public int getTotalFields() {
        return 1;
    }

    @Override
    public Class<Map<String, Object>> getTypeClass() {
        return (Class<Map<String, Object>>) (Class<?>) Map.class;
    }

    @Override
    public boolean isKeyType() {
        return false;
    }

    @Override
    public TypeSerializer<Map<String, Object>> createSerializer(ExecutionConfig config) {
        return serializer;
    }

    @Override
    public String toString() {
        return "MapTypeInformation";
    }

    @Override
    public boolean equals(Object obj) {
        return obj instanceof MapTypeInformation;
    }

    @Override
    public int hashCode() {
        return MapTypeInformation.class.hashCode();
    }
}

// Schema builder for the Flink table
public class DeltaTableSchema {
    private static final Logger LOG = LoggerFactory.getLogger(DeltaTableSchema.class);

    public static Schema buildSchema() {
        LOG.debug("Building table schema for delta table");

        // Define the RAW type for Map<String, Object>
        TypeSerializer<Map<String, Object>> mapSerializer = new MapSerializer();
        DataType rawMapType = DataTypes.RAW(Map.class, mapSerializer);

        return Schema.newBuilder()
                .column("key", DataTypes.STRING())
                .column("checksum", DataTypes.STRING())
                .column("value", rawMapType)
                .column("businessDate", DataTypes.STRING())
                .column("processedDate", DataTypes.STRING())
                .column("version", DataTypes.INT())
                .build();
    }
}

// Data processing logic
public class DataProcessor {
    public static void processData(DataStream<Tuple2<DBRecord, DeltaRecord>> filteredChecksumStream) {
        // Map DeltaRecord to Row
        DataStream<Row> deltaRecordsUpsertStream = filteredChecksumStream
                .map(t -> {
                    DeltaRecord record = t.f1;
                    return Row.of(record.key(), record.checksum(), record.value(), record.businessDate(),
                            record.processedDate(), record.version());
                })
                .returns(Types.ROW_NAMED(
                        new String[]{"KEYCONSTANT", "CHECKSUMCONSTANT", "VALUECONSTANT", "BUSINESSDATECONSTANT",
                                "PROCESSEDDATECONSTANT", "VERSIONCONSTANT"},
                        Types.STRING, Types.STRING, new MapTypeInformation(), Types.STRING, Types.STRING, Types.INT));

        // Convert Row to MongoDB Document
        DataStream<Document> documentStream = deltaRecordsUpsertStream.map(row -> {
            String key = (String) row.getField(0);
            String checksum = (String) row.getField(1);
            Map<String, Object> value = (Map<String, Object>) row.getField(2);
            String businessDate = (String) row.getField(3);
            String processedDate = (String) row.getField(4);
            Integer version = (Integer) row.getField(5);

            return new Document()
                    .append("key", key)
                    .append("checksum", checksum)
                    .append("value", value)
                    .append("businessDate", businessDate)
                    .append("processedDate", processedDate)
                    .append("version", version);
        });

        // Note: Add your MongoDB sink here, e.g., documentStream.addSink(new MongoSink(...));
    }
}

// Placeholder for DBRecord (replace with actual implementation if needed)
class DBRecord {
    // Define fields and methods as per your requirements
}
class Tuple2<T1, T2> {
    public T1 f0;
    public T2 f1;

    public Tuple2(T1 f0, T2 f1) {
        this.f0 = f0;
        this.f1 = f1;
    }
}
