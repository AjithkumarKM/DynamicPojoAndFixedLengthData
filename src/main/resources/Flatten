import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.core.JsonProcessingException;
import java.io.Serializable;
import java.util.Map;

public record DeltaRecord(
    String key,
    String checksum,
    Map<String, Object> value,
    String businessDate,
    String processedDate,
    Integer version
) implements Serializable {
    private static final ObjectMapper MAPPER = new ObjectMapper();

    public static DeltaRecord fromStringValue(
        String key,
        String checksum,
        String value,
        String businessDate,
        String processedDate,
        Integer version
    ) {
        if (value == null) {
            throw new IllegalArgumentException("Value cannot be null");
        }
        try {
            Map<String, Object> valueMap = MAPPER.readValue(value, Map.class);
            return new DeltaRecord(key, checksum, valueMap, businessDate, processedDate, version);
        } catch (JsonProcessingException e) {
            throw new IllegalArgumentException("Failed to parse value as JSON: " + value, e);
        }
    }
}

---------
import org.apache.flink.table.api.DataTypes;
import org.apache.flink.table.api.Schema;
import org.apache.flink.table.types.DataType;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.common.typeutils.TypeSerializer;
import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
import org.apache.flink.core.memory.DataInputView;
import org.apache.flink.core.memory.DataOutputView;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.core.JsonProcessingException;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

public static Schema buildSchema() {
    LOG.debug("Building table schema for delta table");

    // Define TypeInformation for Map<String, Object>
    TypeInformation<Map<String, Object>> mapTypeInfo = Types.MAP(Types.STRING, Types.OBJECT);

    // Create a custom TypeSerializer for Map<String, Object>
    TypeSerializer<Map<String, Object>> mapSerializer = new MapSerializer();

    // Define the RAW type using the serializer
    DataType rawMapType = DataTypes.RAW(Map.class, mapSerializer);

    return Schema.newBuilder()
        .column("key", DataTypes.STRING())
        .column("checksum", DataTypes.STRING())
        .column("value", rawMapType)
        .column("businessDate", DataTypes.STRING())
        .column("processedDate", DataTypes.STRING())
        .column("version", DataTypes.INT())
        .build();
}

// Custom TypeSerializer for Map<String, Object>
class MapSerializer extends TypeSerializerSingleton<Map<String, Object>> {
    private static final ObjectMapper MAPPER = new ObjectMapper();

    @Override
    public boolean isImmutableType() {
        return false;
    }

    @Override
    public Map<String, Object> createInstance() {
        return new HashMap<>();
    }

    @Override
    public Map<String, Object> copy(Map<String, Object> from) {
        return new HashMap<>(from);
    }

    @Override
    public Map<String, Object> copy(Map<String, Object> from, Map<String, Object> reuse) {
        reuse.clear();
        reuse.putAll(from);
        return reuse;
    }

    @Override
    public int getLength() {
        return -1; // Variable length
    }

    @Override
    public void serialize(Map<String, Object> value, DataOutputView target) throws IOException {
        if (value == null) {
            throw new IllegalArgumentException("Cannot serialize null Map");
        }
        try {
            byte[] bytes = MAPPER.writeValueAsBytes(value);
            target.writeInt(bytes.length);
            target.write(bytes);
        } catch (JsonProcessingException e) {
            throw new IOException("Failed to serialize Map to JSON bytes: " + value, e);
        }
    }

    @Override
    public Map<String, Object> deserialize(DataInputView source) throws IOException {
        int length = source.readInt();
        byte[] bytes = new byte[length];
        source.readFully(bytes);
        try {
            return MAPPER.readValue(bytes, Map.class);
        } catch (Exception e) {
            throw new IOException("Failed to deserialize bytes to Map", e);
        }
    }

    @Override
    public Map<String, Object> deserialize(Map<String, Object> reuse, DataInputView source) throws IOException {
        reuse.clear();
        Map<String, Object> result = deserialize(source);
        reuse.putAll(result);
        return reuse;
    }

    @Override
    public void copy(DataInputView source, DataOutputView target) throws IOException {
        int length = source.readInt();
        target.writeInt(length);
        byte[] bytes = new byte[length];
        source.readFully(bytes);
        target.write(bytes);
    }

    @Override
    public TypeSerializerSnapshot<Map<String, Object>> snapshotConfiguration() {
        return new MapSerializerSnapshot();
    }
}

// TypeSerializerSnapshot for MapSerializer
class MapSerializerSnapshot implements TypeSerializerSnapshot<Map<String, Object>> {
    @Override
    public int getCurrentVersion() {
        return 1;
    }

    @Override
    public void writeSnapshot(DataOutputView out) throws IOException {
        // No state to write
    }

    @Override
    public void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {
        // No state to read
    }

    @Override
    public TypeSerializer<Map<String, Object>> restoreSerializer() {
        return new MapSerializer();
    }
}


--------
import org.apache.flink.types.Row;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.bson.Document;

DataStream<Row> deltaRecordsUpsertStream = filteredChecksumStream
    .map(t -> {
        DeltaRecord record = t.f1; // Already a DeltaRecord with value as Map<String, Object>
        return Row.of(record.key(), record.checksum(), record.value(), record.businessDate(), record.processedDate(), record.version());
    })
    .returns(Types.ROW_NAMED(
        new String[]{"KEYCONSTANT", "CHECKSUMCONSTANT", "VALUECONSTANT", "BUSINESSDATECONSTANT", "PROCESSEDDATECONSTANT", "VERSIONCONSTANT"},
        Types.STRING, Types.STRING, Types.OBJECT, Types.STRING, Types.STRING, Types.INT));

// Convert Row to Document for MongoDB sink
DataStream<Document> documentStream = deltaRecordsUpsertStream.map(row -> {
    String key = (String) row.getField(0);
    String checksum = (String) row.getField(1);
    Map<String, Object> value = (Map<String, Object>) row.getField(2);
    String businessDate = (String) row.getField(3);
    String processedDate = (String) row.getField(4);
    Integer version = (Integer) row.getField(5);

    return new Document()
        .append("key", key)
        .append("checksum", checksum)
        .append("value", value)
        .append("businessDate", businessDate)
        .append("processedDate", processedDate)
        .append("version", version);
});

// Use the MongoDB sink to insert the Document
// (Assuming a MongoDB sink is configured, e.g., MongoSink)
// statementSet.add(fileUpsertTable.insertInto(s: collectionName + SINK_VAR));
